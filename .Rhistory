unnest(json)
train[1:100, ] %>%
mutate(json = map(device, ~ fromJSON(.) %>% as.data.frame()))
train[1:100, ] %>%
mutate(json = map(device, ~ fromJSON(.)))
train[1:100, ] %>%
mutate(json = map(device, ~ fromJSON(.))) %>%
unnest(json)
train[1:100, ] %>%
mutate(json = map(device, ~ fromJSON(.))) %>%
unnest(json) %>% View
train[1:100, ] %>%
mutate(json = map(device, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>% View
train[1:100, ] %>% head
mutate(json = map(device, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>% View
train[1:100, ] %>%
mutate(json = map(device, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>% View
warnings()
train[1:100, ] %>%
mutate(json = map(device, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>% colanes
train[1:100, ] %>%
mutate(json = map(device, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>% colnames
train[1:100, ] %>% head
train[1:100, ] %>%
mutate(json = map(device, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
mutate(json = map(geoNetwork, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
mutate(json = map(totals, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
mutate(json = map(trafficSource, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
select(-device, -geoNetwork, -totals, -trafficSource) %>% colnames
train %<>%
mutate(json = map(device, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
mutate(json = map(geoNetwork, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
mutate(json = map(totals, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
mutate(json = map(trafficSource, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
select(-device, -geoNetwork, -totals, -trafficSource) %>% colnames
train %<>%
mutate(json = map(device, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
mutate(json = map(geoNetwork, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
mutate(json = map(totals, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
mutate(json = map(trafficSource, ~ fromJSON(.) %>% as.data.frame())) %>%
unnest(json) %>%
select(-device, -geoNetwork, -totals, -trafficSource)
train %<>%
mutate(json1 = map(device, ~ fromJSON(.) %>% as.data.frame())) %>%
mutate(json2 = map(geoNetwork, ~ fromJSON(.) %>% as.data.frame())) %>%
mutate(json3 = map(totals, ~ fromJSON(.) %>% as.data.frame())) %>%
mutate(json4 = map(trafficSource, ~ fromJSON(.) %>% as.data.frame())) %>%
select(-device, -geoNetwork, -totals, -trafficSource)
View(train)
colnames(train)
train <- read_csv(paste0(data_path, "train.csv"))
test <- read_csv(paste0(data_path, "test.csv"))
n <- Sys.time() #See how long this takes to run
#JSON columns are "device", "geoNetwork", "totals", "trafficSource"
tr_device <- paste("[", paste(train$device, collapse = ","), "]") %>% fromJSON(flatten = T)
tr_geoNetwork <- paste("[", paste(train$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
tr_totals <- paste("[", paste(train$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
tr_trafficSource <- paste("[", paste(train$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)
te_device <- paste("[", paste(test$device, collapse = ","), "]") %>% fromJSON(flatten = T)
te_geoNetwork <- paste("[", paste(test$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
te_totals <- paste("[", paste(test$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
te_trafficSource <- paste("[", paste(test$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)
#Check to see if the training and test sets have the same column names
setequal(names(tr_device), names(te_device))
setequal(names(tr_geoNetwork), names(te_geoNetwork))
setequal(names(tr_totals), names(te_totals))
setequal(names(tr_trafficSource), names(te_trafficSource))
#As expected, tr_totals and te_totals are different as the train set includes the target, transactionRevenue
names(tr_totals)
names(te_totals)
#Apparently tr_trafficSource contains an extra column as well - campaignCode
#It actually has only one non-NA value, so this column can safely be dropped later
table(tr_trafficSource$campaignCode, exclude = NULL)
names(tr_trafficSource)
names(te_trafficSource)
#Combine to make the full training and test sets
train <- train %>%
cbind(tr_device, tr_geoNetwork, tr_totals, tr_trafficSource) %>%
select(-device, -geoNetwork, -totals, -trafficSource)
test <- test %>%
cbind(te_device, te_geoNetwork, te_totals, te_trafficSource) %>%
select(-device, -geoNetwork, -totals, -trafficSource)
#Number of columns in the new training and test sets.
ncol(train)
ncol(test)
#Remove temporary tr_ and te_ sets
rm(tr_device); rm(tr_geoNetwork); rm(tr_totals); rm(tr_trafficSource)
rm(te_device); rm(te_geoNetwork); rm(te_totals); rm(te_trafficSource)
#How long did this script take?
Sys.time() - n
train <- read_csv(paste0(data_path, "train.csv"))
n <- Sys.time() #See how long this takes to run
train %>%
mutate(json1 = map(device, ~ fromJSON(.) %>% as.data.frame())) %>%
mutate(json2 = map(geoNetwork, ~ fromJSON(.) %>% as.data.frame())) %>%
mutate(json3 = map(totals, ~ fromJSON(.) %>% as.data.frame())) %>%
mutate(json4 = map(trafficSource, ~ fromJSON(.) %>% as.data.frame())) %>%
select(-device, -geoNetwork, -totals, -trafficSource) -> dr
library(tidyverse)
library(jsonlite)
data_path <- "/home/michal/Documents/hdd/google_analytics/"
train <- read_csv(paste0(data_path, "train.csv"))
test <- read_csv(paste0(data_path, "test.csv"))
tr_device <- paste("[", paste(train$device, collapse = ","), "]") %>% fromJSON(flatten = T)
tr_geoNetwork <- paste("[", paste(train$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
tr_totals <- paste("[", paste(train$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
tr_trafficSource <- paste("[", paste(train$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)
te_device <- paste("[", paste(test$device, collapse = ","), "]") %>% fromJSON(flatten = T)
te_geoNetwork <- paste("[", paste(test$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
te_totals <- paste("[", paste(test$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
te_trafficSource <- paste("[", paste(test$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)
train <- train %>%
cbind(tr_device, tr_geoNetwork, tr_totals, tr_trafficSource) %>%
select(-device, -geoNetwork, -totals, -trafficSource)
test <- test %>%
cbind(te_device, te_geoNetwork, te_totals, te_trafficSource) %>%
select(-device, -geoNetwork, -totals, -trafficSource)
rm(tr_device); rm(tr_geoNetwork); rm(tr_totals); rm(tr_trafficSource)
rm(te_device); rm(te_geoNetwork); rm(te_totals); rm(te_trafficSource)
write.csv(train, paste0(data_path, "train_flat.csv"), row.names = F)
write.csv(test, paste0(data_path, "test_flat.csv"), row.names = F)
library(tidyverse)
library(magrittr)
read.table("data_path")
readChar("data_path", file.info("data_path")$size)
fr <- readChar("data_path", file.info("data_path")$size)
fr
read_file("data_path")
data_path <- read.table("data_path", text = T)
data_path <- read.table("data_path", text = F)
scan("data_path")
scan("data_path", what = "character")
data_path <- scan("data_path", what = "character")
rm(fr)
train <- read_csv(paste0(data_path, "train_flat.csv"))
test <- read_csv(paste0(data_path, "test_flat.csv"))
View(test)
View(train)
?log
colnames(train)
head(train)
hist(train$transactionRevenue)
hist(train$transactionRevenue, breaks = 200)
setdiff(colnames(train), colnames(test))
table(train$campaignCode)
train <- train[, -"campaignCode"]
train %<>% select(-campaignCode)
#predicted value - natural logarithm of transactionRevenue column
train %>%
mutate(transactionRevenue = ifelse(is.na(transactionRevenue), 0 , transactionRevenue))
#predicted value - natural logarithm of transactionRevenue column
train %<>%
mutate(transactionRevenue = ifelse(is.na(transactionRevenue), 0, transactionRevenue))
ggplot(train, aes("transactionRevenue"))+
geom_histogram()
ggplot(train)+
geom_histogram(aes(x = "transactionRevenue"))
class(train$transactionRevenue)
hist(train$transactionRevenue)
hist(train$transactionRevenue, breaks = 1000)
hist(train$transactionRevenue, breaks = 100)
hist(train$transactionRevenue, breaks = 100, xlim = c(0, 10000))
hist(train$transactionRevenue, breaks = 1000, xlim = c(0, 10000))
hist(train$transactionRevenue, breaks = 10000, xlim = c(0, 10000))
table(train$transactionRevenue)
train <- read_csv(paste0(data_path, "train_flat.csv"))
test <- read_csv(paste0(data_path, "test_flat.csv"))
setdiff(colnames(train), colnames(test))
table(train$campaignCode)
train %<>% select(-campaignCode)
#predicted value - natural logarithm of transactionRevenue column
train %<>%
mutate(transactionRevenue = ifelse(is.na(transactionRevenue), 0, transactionRevenue)) %>%
mutate(PredictedLogRevenue = log(transactionRevenue))
hist(train$PredictedLogRevenue)
sapply(train, class)
head(train)
colnames(train)
train %>%
mutate_at(is.character(.), factor)
?mutate_at
train %>%
mutate_at(is.character, factor)
train %>%
mutate_if(is.character, factor)
train %<>%
mutate_if(is.character, factor)
test %<>%
mutate_if(is.character, factor)
View(train)
library(tidyverse)
library(magrittr)
data_path <- scan("data_path", what = "character")
train <- read_csv(paste0(data_path, "train_flat.csv"))
test <- read_csv(paste0(data_path, "test_flat.csv"))
setdiff(colnames(train), colnames(test))
table(train$campaignCode)
train %<>% select(-campaignCode)
#predicted value - natural logarithm of transactionRevenue column
train %<>%
mutate(transactionRevenue = ifelse(is.na(transactionRevenue), 0, transactionRevenue)) %>%
mutate(PredictedLogRevenue = log(transactionRevenue))
head(train)
colnames(train)
train %<>%
mutate_if(is.character, factor)
test %<>%
mutate_if(is.character, factor)
id_cols <- c("fullVisitorId", "sessionId", "visitId")
train$networkDomain
sapply(train, function(x) ifelse(is.factor(x), length(levels(x)), 0))
#number of levels for factor columns
sapply(train, function(x) ifelse(is.factor(x), length(levels(x)), 0))
#number of levels for factor columns
nlevels <- sapply(train, function(x) ifelse(is.factor(x), length(levels(x)), 0))
colnames(train)[which(nlevels > 100)]
#id columns
id_cols <- c("fullVisitorId", "sessionId", "visitId")
setdiff(colnames(train)[which(nlevels > 100)], id_cols)
too_many_lvls_cols <- setdiff(colnames(train)[which(nlevels > 100)], id_cols)
networkDomain - to remove?
train$visitStartTime
networkDomain - to remove?
as.POSIXct(train$visitStartTime, origin = "1970-01-01")
colnames(train)
train$date
ymd(train$date)
library(lubridate)
ymd(train$date)
networkDomain - to remove?
train %<>%
mutate(visitStartTime = as.POSIXct(visitStartTime, origin = "1970-01-01"),
date = ymd(date))
test %<>%
mutate(visitStartTime = as.POSIXct(visitStartTime, origin = "1970-01-01"),
date = ymd(date))
too_many_lvls_cols
View(test)
unnecessary_cols <- c("visitStartTime")
#check for columns containg a lot of missing data
sapply(train, function(x) round(sum(is.na(x))/nrow(train)*100, 2))
#check for columns containg a lot of missing data
percentMissing <- sapply(train, function(x) round(sum(is.na(x))/nrow(train)*100, 2))
#check for columns containg a lot of missing data
percent_missing <- sapply(train, function(x) round(sum(is.na(x))/nrow(train)*100, 2))
sort(percent_missing)
colnames(train)[which(percent_missing > 75)]
missing_val_cols <- colnames(train)[which(percent_missing > 75)]
too_many_lvls_cols
unnecessary_cols
missing_val_cols
?setdiff
union(c(1, 2), c(2, 3))
unique(c(too_many_lvls_cols, unnecessary_cols, missing_val_cols))
to_remove <- unique(c(too_many_lvls_cols, unnecessary_cols, missing_val_cols))
train %<>%
select(-one_of(to_remove))
test %<>%
select(-one_of(to_remove))
setdiff(colnames(train), colnames(test))
rm(list = ls())
library(tidyverse)
library(magrittr)
library(lubridate)
data_path <- scan("data_path", what = "character")
train <- read_csv(paste0(data_path, "train_flat.csv"))
test <- read_csv(paste0(data_path, "test_flat.csv"))
setdiff(colnames(train), colnames(test))
table(train$campaignCode)
train %<>% select(-campaignCode)
#predicted value - natural logarithm of transactionRevenue column
train %<>%
mutate(transactionRevenue = ifelse(is.na(transactionRevenue), 0, transactionRevenue)) %>%
mutate(PredictedLogRevenue = log(transactionRevenue))
#convert character columns to factors
train %<>%
mutate_if(is.character, factor)
test %<>%
mutate_if(is.character, factor)
#id columns
id_cols <- c("fullVisitorId", "sessionId", "visitId")
#number of levels for factor columns
nlevels <- sapply(train, function(x) ifelse(is.factor(x), length(levels(x)), 0))
#which columns should be removed due to too many levels (better address the problem than remove them)
too_many_lvls_cols <- setdiff(colnames(train)[which(nlevels > 100)], id_cols)
#columns containing dates
train %<>%
mutate(visitStartTime = as.POSIXct(visitStartTime, origin = "1970-01-01"),
date = ymd(date))
test %<>%
mutate(visitStartTime = as.POSIXct(visitStartTime, origin = "1970-01-01"),
date = ymd(date))
unnecessary_cols <- c("visitStartTime", "transactionRevenue")
#check for columns containg a lot of missing data
percent_missing <- sapply(train, function(x) round(sum(is.na(x))/nrow(train)*100, 2))
missing_val_cols <- colnames(train)[which(percent_missing > 75)]
to_remove <- unique(c(too_many_lvls_cols, unnecessary_cols, missing_val_cols))
train %<>%
select(-one_of(to_remove))
test %<>%
select(-one_of(to_remove))
readRDS(train, paste0(data_path, "train_clean.rds"))
writeRDS(train, paste0(data_path, "train_clean.rds"))
saveRDS(train, paste0(data_path, "train_clean.rds"))
saveRDS(test, paste0(data_path, "test_clean.rds"))
rm(list = ls())
library(magrittr)
library(randomForest)
data_path <- scan("data_path", what = "character")
data_path <- scan("data_path", what = "character")
train <- readRDS(paste0(data_path, "train_clean.rds"))
test <- readRDS(paste0(data_path, "test_clean.rds"))
colnames(train)
rf <- randomForest(PredictedLogRevenue ~ ., data = train[, -id_cols],
ntree = 50)
#id columns
id_cols <- c("fullVisitorId", "sessionId", "visitId")
rf <- randomForest(PredictedLogRevenue ~ ., data = train[, -id_cols],
ntree = 50)
rf <- randomForest(PredictedLogRevenue ~ ., data = train[, -which(colnames(train) %in% id_cols)],
ntree = 50)
library(tidyverse)
library(magrittr)
library(lubridate)
data_path <- scan("data_path", what = "character")
train <- read_csv(paste0(data_path, "train_flat.csv"))
test <- read_csv(paste0(data_path, "test_flat.csv"))
setdiff(colnames(train), colnames(test))
table(train$campaignCode)
train %<>% select(-campaignCode)
#predicted value - natural logarithm of transactionRevenue column
train %>%
mutate(PredictedLogRevenue = log(transactionRevenue))
#predicted value - natural logarithm of transactionRevenue column
train %>%
mutate(PredictedLogRevenue = log(transactionRevenue)) -> fr
fr$PredictedLogRevenue
#predicted value - natural logarithm of transactionRevenue column
train %<>%
mutate(PredictedLogRevenue = log(transactionRevenue)) %>%
mutate(PredictedLogRevenue = ifelse(is.na(PredictedLogRevenue), 0, transactionRevenue))
train$PredictedLogRevenue
train <- read_csv(paste0(data_path, "train_flat.csv"))
test <- read_csv(paste0(data_path, "test_flat.csv"))
setdiff(colnames(train), colnames(test))
table(train$campaignCode)
train %<>% select(-campaignCode)
#predicted value - natural logarithm of transactionRevenue column
train %<>%
mutate(PredictedLogRevenue = log(transactionRevenue)) %>%
mutate(PredictedLogRevenue = ifelse(is.na(PredictedLogRevenue), 0, PredictedLogRevenue))
#convert character columns to factors
train %<>%
mutate_if(is.character, factor)
test %<>%
mutate_if(is.character, factor)
#id columns
id_cols <- c("fullVisitorId", "sessionId", "visitId")
#number of levels for factor columns
nlevels <- sapply(train, function(x) ifelse(is.factor(x), length(levels(x)), 0))
#which columns should be removed due to too many levels (better address the problem than remove them)
too_many_lvls_cols <- setdiff(colnames(train)[which(nlevels > 100)], id_cols)
#columns containing dates
train %<>%
mutate(visitStartTime = as.POSIXct(visitStartTime, origin = "1970-01-01"),
date = ymd(date))
test %<>%
mutate(visitStartTime = as.POSIXct(visitStartTime, origin = "1970-01-01"),
date = ymd(date))
unnecessary_cols <- c("visitStartTime", "transactionRevenue")
#check for columns containg a lot of missing data
percent_missing <- sapply(train, function(x) round(sum(is.na(x))/nrow(train)*100, 2))
missing_val_cols <- colnames(train)[which(percent_missing > 75)]
to_remove <- unique(c(too_many_lvls_cols, unnecessary_cols, missing_val_cols))
train %<>%
select(-one_of(to_remove))
test %<>%
select(-one_of(to_remove))
saveRDS(train, paste0(data_path, "train_clean.rds"))
saveRDS(test, paste0(data_path, "test_clean.rds"))
rm(list = ls())
library(tidyverse)
library(magrittr)
library(randomForest)
data_path <- scan("data_path", what = "character")
train <- readRDS(paste0(data_path, "train_clean.rds"))
test <- readRDS(paste0(data_path, "test_clean.rds"))
#id columns
id_cols <- c("fullVisitorId", "sessionId", "visitId")
rf <- randomForest(PredictedLogRevenue ~ ., data = train[, -which(colnames(train) %in% id_cols)],
ntree = 50)
sum(is.na(train$PredictedLogRevenue))
library(tidyverse)
library(magrittr)
library(lubridate)
data_path <- scan("data_path", what = "character")
train <- read_csv(paste0(data_path, "train_flat.csv"))
test <- read_csv(paste0(data_path, "test_flat.csv"))
setdiff(colnames(train), colnames(test))
table(train$campaignCode)
train %<>% select(-campaignCode)
#predicted value - natural logarithm of transactionRevenue column
train %<>%
mutate(PredictedLogRevenue = log(transactionRevenue)) %>%
mutate(PredictedLogRevenue = ifelse(is.na(PredictedLogRevenue), 0, PredictedLogRevenue))
#convert character columns to factors
train %<>%
mutate_if(is.character, factor)
test %<>%
mutate_if(is.character, factor)
#id columns
id_cols <- c("fullVisitorId", "sessionId", "visitId")
#number of levels for factor columns
nlevels <- sapply(train, function(x) ifelse(is.factor(x), length(levels(x)), 0))
#which columns should be removed due to too many levels (better address the problem than remove them)
too_many_lvls_cols <- setdiff(colnames(train)[which(nlevels > 100)], id_cols)
#columns containing dates
train %<>%
mutate(visitStartTime = as.POSIXct(visitStartTime, origin = "1970-01-01"),
date = ymd(date))
test %<>%
mutate(visitStartTime = as.POSIXct(visitStartTime, origin = "1970-01-01"),
date = ymd(date))
unnecessary_cols <- c("visitStartTime", "transactionRevenue")
#check for columns containg a lot of missing data
percent_missing <- sapply(train, function(x) round(sum(is.na(x))/nrow(train)*100, 2))
missing_val_cols <- colnames(train)[which(percent_missing > 0)]
to_remove <- unique(c(too_many_lvls_cols, unnecessary_cols, missing_val_cols))
train %<>%
select(-one_of(to_remove))
test %<>%
select(-one_of(to_remove))
saveRDS(train, paste0(data_path, "train_clean.rds"))
saveRDS(test, paste0(data_path, "test_clean.rds"))
data_path <- scan("data_path", what = "character")
train <- readRDS(paste0(data_path, "train_clean.rds"))
test <- readRDS(paste0(data_path, "test_clean.rds"))
#id columns
id_cols <- c("fullVisitorId", "sessionId", "visitId")
rf <- randomForest(PredictedLogRevenue ~ ., data = train[, -which(colnames(train) %in% id_cols)],
ntree = 50)
library(tidyverse)
library(magrittr)
library(lubridate)
data_path <- scan("data_path", what = "character")
train <- read_csv(paste0(data_path, "train_flat.csv"))
test <- read_csv(paste0(data_path, "test_flat.csv"))
setdiff(colnames(train), colnames(test))
table(train$campaignCode)
train %<>% select(-campaignCode)
#predicted value - natural logarithm of transactionRevenue column
train %<>%
mutate(PredictedLogRevenue = log(transactionRevenue)) %>%
mutate(PredictedLogRevenue = ifelse(is.na(PredictedLogRevenue), 0, PredictedLogRevenue))
#convert character columns to factors
train %<>%
mutate_if(is.character, factor)
test %<>%
mutate_if(is.character, factor)
#id columns
id_cols <- c("fullVisitorId", "sessionId", "visitId")
#number of levels for factor columns
nlevels <- sapply(train, function(x) ifelse(is.factor(x), length(levels(x)), 0))
#which columns should be removed due to too many levels (better address the problem than remove them)
too_many_lvls_cols <- setdiff(colnames(train)[which(nlevels > 50)], id_cols)
#columns containing dates
train %<>%
mutate(visitStartTime = as.POSIXct(visitStartTime, origin = "1970-01-01"),
date = ymd(date))
test %<>%
mutate(visitStartTime = as.POSIXct(visitStartTime, origin = "1970-01-01"),
date = ymd(date))
unnecessary_cols <- c("visitStartTime", "transactionRevenue")
#check for columns containg a lot of missing data
percent_missing <- sapply(train, function(x) round(sum(is.na(x))/nrow(train)*100, 2))
missing_val_cols <- colnames(train)[which(percent_missing > 0)]
to_remove <- unique(c(too_many_lvls_cols, unnecessary_cols, missing_val_cols))
train %<>%
select(-one_of(to_remove))
test %<>%
select(-one_of(to_remove))
saveRDS(train, paste0(data_path, "train_clean.rds"))
saveRDS(test, paste0(data_path, "test_clean.rds"))
train <- readRDS(paste0(data_path, "train_clean.rds"))
test <- readRDS(paste0(data_path, "test_clean.rds"))
#id columns
id_cols <- c("fullVisitorId", "sessionId", "visitId")
rf <- randomForest(PredictedLogRevenue ~ ., data = train[, -which(colnames(train) %in% id_cols)],
ntree = 50)
plot(rd)
plot(rf)
rf
importance(rf)
plot(importance(rf))
predict(rf, test[, -which(colnames(train) %in% id_cols)])
predict(rf, test[, -which(colnames(test) %in% id_cols)])
